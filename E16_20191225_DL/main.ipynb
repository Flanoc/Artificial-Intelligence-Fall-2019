{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 24500) loss: 2.304606\n",
      "(Epoch 0 / 50) train acc: 0.117000; val_acc: 0.105000\n",
      "(Iteration 21 / 24500) loss: 1.955501\n",
      "(Iteration 41 / 24500) loss: 1.757391\n",
      "(Iteration 61 / 24500) loss: 1.776934\n",
      "(Iteration 81 / 24500) loss: 1.670851\n",
      "(Iteration 101 / 24500) loss: 1.526764\n",
      "(Iteration 121 / 24500) loss: 1.580357\n",
      "(Iteration 141 / 24500) loss: 1.397806\n",
      "(Iteration 161 / 24500) loss: 1.602323\n",
      "(Iteration 181 / 24500) loss: 1.541478\n",
      "(Iteration 201 / 24500) loss: 1.718813\n",
      "(Iteration 221 / 24500) loss: 1.658825\n",
      "(Iteration 241 / 24500) loss: 1.382261\n",
      "(Iteration 261 / 24500) loss: 1.492840\n",
      "(Iteration 281 / 24500) loss: 1.827655\n",
      "(Iteration 301 / 24500) loss: 1.586555\n",
      "(Iteration 321 / 24500) loss: 1.497718\n",
      "(Iteration 341 / 24500) loss: 1.578420\n",
      "(Iteration 361 / 24500) loss: 1.420902\n",
      "(Iteration 381 / 24500) loss: 1.326502\n",
      "(Iteration 401 / 24500) loss: 1.432076\n",
      "(Iteration 421 / 24500) loss: 1.527987\n",
      "(Iteration 441 / 24500) loss: 1.585634\n",
      "(Iteration 461 / 24500) loss: 1.431846\n",
      "(Iteration 481 / 24500) loss: 1.392239\n",
      "(Epoch 1 / 50) train acc: 0.516000; val_acc: 0.524000\n",
      "(Iteration 501 / 24500) loss: 1.452853\n",
      "(Iteration 521 / 24500) loss: 1.511839\n",
      "(Iteration 541 / 24500) loss: 1.445927\n",
      "(Iteration 561 / 24500) loss: 1.325781\n",
      "(Iteration 581 / 24500) loss: 1.472048\n",
      "(Iteration 601 / 24500) loss: 1.736038\n",
      "(Iteration 621 / 24500) loss: 1.294852\n",
      "(Iteration 641 / 24500) loss: 1.472182\n",
      "(Iteration 661 / 24500) loss: 1.498297\n",
      "(Iteration 681 / 24500) loss: 1.305143\n",
      "(Iteration 701 / 24500) loss: 1.354921\n",
      "(Iteration 721 / 24500) loss: 1.330922\n",
      "(Iteration 741 / 24500) loss: 1.201885\n",
      "(Iteration 761 / 24500) loss: 1.310348\n",
      "(Iteration 781 / 24500) loss: 1.439985\n",
      "(Iteration 801 / 24500) loss: 1.398497\n",
      "(Iteration 821 / 24500) loss: 1.412158\n",
      "(Iteration 841 / 24500) loss: 1.249347\n",
      "(Iteration 861 / 24500) loss: 1.178713\n",
      "(Iteration 881 / 24500) loss: 1.354026\n",
      "(Iteration 901 / 24500) loss: 1.378002\n",
      "(Iteration 921 / 24500) loss: 1.221234\n",
      "(Iteration 941 / 24500) loss: 1.153838\n",
      "(Iteration 961 / 24500) loss: 1.040962\n",
      "(Epoch 2 / 50) train acc: 0.555000; val_acc: 0.558000\n",
      "(Iteration 981 / 24500) loss: 1.288763\n",
      "(Iteration 1001 / 24500) loss: 1.313664\n",
      "(Iteration 1021 / 24500) loss: 1.197566\n",
      "(Iteration 1041 / 24500) loss: 1.316995\n",
      "(Iteration 1061 / 24500) loss: 1.740613\n",
      "(Iteration 1081 / 24500) loss: 1.461498\n",
      "(Iteration 1101 / 24500) loss: 1.220026\n",
      "(Iteration 1121 / 24500) loss: 1.397569\n",
      "(Iteration 1141 / 24500) loss: 1.114041\n",
      "(Iteration 1161 / 24500) loss: 1.233544\n",
      "(Iteration 1181 / 24500) loss: 1.325866\n",
      "(Iteration 1201 / 24500) loss: 1.170986\n",
      "(Iteration 1221 / 24500) loss: 1.504520\n",
      "(Iteration 1241 / 24500) loss: 0.958013\n",
      "(Iteration 1261 / 24500) loss: 1.181082\n",
      "(Iteration 1281 / 24500) loss: 1.281254\n",
      "(Iteration 1301 / 24500) loss: 1.213033\n",
      "(Iteration 1321 / 24500) loss: 1.223382\n",
      "(Iteration 1341 / 24500) loss: 1.142405\n",
      "(Iteration 1361 / 24500) loss: 1.102579\n",
      "(Iteration 1381 / 24500) loss: 1.351290\n",
      "(Iteration 1401 / 24500) loss: 1.311039\n",
      "(Iteration 1421 / 24500) loss: 1.162899\n",
      "(Iteration 1441 / 24500) loss: 1.102847\n",
      "(Iteration 1461 / 24500) loss: 1.209829\n",
      "(Epoch 3 / 50) train acc: 0.603000; val_acc: 0.572000\n",
      "(Iteration 1481 / 24500) loss: 1.031995\n",
      "(Iteration 1501 / 24500) loss: 1.161364\n",
      "(Iteration 1521 / 24500) loss: 1.449565\n",
      "(Iteration 1541 / 24500) loss: 1.119639\n",
      "(Iteration 1561 / 24500) loss: 1.142033\n",
      "(Iteration 1581 / 24500) loss: 1.170044\n",
      "(Iteration 1601 / 24500) loss: 1.248218\n",
      "(Iteration 1621 / 24500) loss: 1.312132\n",
      "(Iteration 1641 / 24500) loss: 1.171334\n",
      "(Iteration 1661 / 24500) loss: 1.307411\n",
      "(Iteration 1681 / 24500) loss: 1.267707\n",
      "(Iteration 1701 / 24500) loss: 0.972344\n",
      "(Iteration 1721 / 24500) loss: 1.363076\n",
      "(Iteration 1741 / 24500) loss: 1.263309\n",
      "(Iteration 1761 / 24500) loss: 1.231879\n",
      "(Iteration 1781 / 24500) loss: 1.104968\n",
      "(Iteration 1801 / 24500) loss: 0.947376\n",
      "(Iteration 1821 / 24500) loss: 1.131554\n",
      "(Iteration 1841 / 24500) loss: 1.140067\n",
      "(Iteration 1861 / 24500) loss: 1.548652\n",
      "(Iteration 1881 / 24500) loss: 1.079338\n",
      "(Iteration 1901 / 24500) loss: 1.199266\n",
      "(Iteration 1921 / 24500) loss: 1.223953\n",
      "(Iteration 1941 / 24500) loss: 1.133535\n",
      "(Epoch 4 / 50) train acc: 0.624000; val_acc: 0.573000\n",
      "(Iteration 1961 / 24500) loss: 1.108432\n",
      "(Iteration 1981 / 24500) loss: 1.138866\n",
      "(Iteration 2001 / 24500) loss: 1.269957\n",
      "(Iteration 2021 / 24500) loss: 1.102563\n",
      "(Iteration 2041 / 24500) loss: 1.234125\n",
      "(Iteration 2061 / 24500) loss: 1.222140\n",
      "(Iteration 2081 / 24500) loss: 1.369591\n",
      "(Iteration 2101 / 24500) loss: 1.224469\n",
      "(Iteration 2121 / 24500) loss: 1.166240\n",
      "(Iteration 2141 / 24500) loss: 1.023085\n",
      "(Iteration 2161 / 24500) loss: 1.012247\n",
      "(Iteration 2181 / 24500) loss: 1.153415\n",
      "(Iteration 2201 / 24500) loss: 0.885283\n",
      "(Iteration 2221 / 24500) loss: 1.048709\n",
      "(Iteration 2241 / 24500) loss: 1.172549\n",
      "(Iteration 2261 / 24500) loss: 0.985157\n",
      "(Iteration 2281 / 24500) loss: 0.948525\n",
      "(Iteration 2301 / 24500) loss: 1.058959\n",
      "(Iteration 2321 / 24500) loss: 1.097679\n",
      "(Iteration 2341 / 24500) loss: 0.978266\n",
      "(Iteration 2361 / 24500) loss: 1.264133\n",
      "(Iteration 2381 / 24500) loss: 1.138799\n",
      "(Iteration 2401 / 24500) loss: 0.907973\n",
      "(Iteration 2421 / 24500) loss: 1.060959\n",
      "(Iteration 2441 / 24500) loss: 0.990718\n",
      "(Epoch 5 / 50) train acc: 0.650000; val_acc: 0.593000\n",
      "(Iteration 2461 / 24500) loss: 1.171056\n",
      "(Iteration 2481 / 24500) loss: 1.004422\n",
      "(Iteration 2501 / 24500) loss: 1.054894\n",
      "(Iteration 2521 / 24500) loss: 1.160367\n",
      "(Iteration 2541 / 24500) loss: 1.315541\n",
      "(Iteration 2561 / 24500) loss: 1.135817\n",
      "(Iteration 2581 / 24500) loss: 1.254698\n",
      "(Iteration 2601 / 24500) loss: 1.025414\n",
      "(Iteration 2621 / 24500) loss: 1.067370\n",
      "(Iteration 2641 / 24500) loss: 1.055055\n",
      "(Iteration 2661 / 24500) loss: 1.042598\n",
      "(Iteration 2681 / 24500) loss: 1.013956\n",
      "(Iteration 2701 / 24500) loss: 0.981196\n",
      "(Iteration 2721 / 24500) loss: 1.038413\n",
      "(Iteration 2741 / 24500) loss: 1.192596\n",
      "(Iteration 2761 / 24500) loss: 1.224043\n",
      "(Iteration 2781 / 24500) loss: 1.266192\n",
      "(Iteration 2801 / 24500) loss: 1.164039\n",
      "(Iteration 2821 / 24500) loss: 0.886115\n",
      "(Iteration 2841 / 24500) loss: 0.934917\n",
      "(Iteration 2861 / 24500) loss: 1.150545\n",
      "(Iteration 2881 / 24500) loss: 1.115814\n",
      "(Iteration 2901 / 24500) loss: 1.112139\n",
      "(Iteration 2921 / 24500) loss: 0.999079\n",
      "(Epoch 6 / 50) train acc: 0.671000; val_acc: 0.605000\n",
      "(Iteration 2941 / 24500) loss: 1.198542\n",
      "(Iteration 2961 / 24500) loss: 1.128437\n",
      "(Iteration 2981 / 24500) loss: 1.001582\n",
      "(Iteration 3001 / 24500) loss: 1.380429\n",
      "(Iteration 3021 / 24500) loss: 0.975454\n",
      "(Iteration 3041 / 24500) loss: 1.188824\n",
      "(Iteration 3061 / 24500) loss: 1.035527\n",
      "(Iteration 3081 / 24500) loss: 1.173436\n",
      "(Iteration 3101 / 24500) loss: 1.071958\n",
      "(Iteration 3121 / 24500) loss: 1.037526\n",
      "(Iteration 3141 / 24500) loss: 0.974652\n",
      "(Iteration 3161 / 24500) loss: 1.041777\n",
      "(Iteration 3181 / 24500) loss: 1.198975\n",
      "(Iteration 3201 / 24500) loss: 1.035177\n",
      "(Iteration 3221 / 24500) loss: 0.975803\n",
      "(Iteration 3241 / 24500) loss: 1.178746\n",
      "(Iteration 3261 / 24500) loss: 0.907155\n",
      "(Iteration 3281 / 24500) loss: 0.994239\n",
      "(Iteration 3301 / 24500) loss: 0.994114\n",
      "(Iteration 3321 / 24500) loss: 0.947753\n",
      "(Iteration 3341 / 24500) loss: 1.214925\n",
      "(Iteration 3361 / 24500) loss: 1.057496\n",
      "(Iteration 3381 / 24500) loss: 1.075880\n",
      "(Iteration 3401 / 24500) loss: 1.083482\n",
      "(Iteration 3421 / 24500) loss: 1.124400\n",
      "(Epoch 7 / 50) train acc: 0.629000; val_acc: 0.599000\n",
      "(Iteration 3441 / 24500) loss: 0.977416\n",
      "(Iteration 3461 / 24500) loss: 0.945134\n",
      "(Iteration 3481 / 24500) loss: 1.028131\n",
      "(Iteration 3501 / 24500) loss: 0.849194\n",
      "(Iteration 3521 / 24500) loss: 1.029016\n",
      "(Iteration 3541 / 24500) loss: 1.207067\n",
      "(Iteration 3561 / 24500) loss: 1.138614\n",
      "(Iteration 3581 / 24500) loss: 1.048459\n",
      "(Iteration 3601 / 24500) loss: 1.077737\n",
      "(Iteration 3621 / 24500) loss: 0.966470\n",
      "(Iteration 3641 / 24500) loss: 0.963383\n",
      "(Iteration 3661 / 24500) loss: 1.059937\n",
      "(Iteration 3681 / 24500) loss: 1.041813\n",
      "(Iteration 3701 / 24500) loss: 0.824561\n",
      "(Iteration 3721 / 24500) loss: 1.038938\n",
      "(Iteration 3741 / 24500) loss: 0.884834\n",
      "(Iteration 3761 / 24500) loss: 1.017648\n",
      "(Iteration 3781 / 24500) loss: 0.919452\n",
      "(Iteration 3801 / 24500) loss: 1.028764\n",
      "(Iteration 3821 / 24500) loss: 1.070742\n",
      "(Iteration 3841 / 24500) loss: 1.019389\n",
      "(Iteration 3861 / 24500) loss: 0.805132\n",
      "(Iteration 3881 / 24500) loss: 0.989572\n",
      "(Iteration 3901 / 24500) loss: 0.993227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8 / 50) train acc: 0.681000; val_acc: 0.635000\n",
      "(Iteration 3921 / 24500) loss: 1.097178\n",
      "(Iteration 3941 / 24500) loss: 1.098508\n",
      "(Iteration 3961 / 24500) loss: 1.006196\n",
      "(Iteration 3981 / 24500) loss: 1.026601\n",
      "(Iteration 4001 / 24500) loss: 1.012370\n",
      "(Iteration 4021 / 24500) loss: 0.955940\n",
      "(Iteration 4041 / 24500) loss: 1.025090\n",
      "(Iteration 4061 / 24500) loss: 1.080588\n",
      "(Iteration 4081 / 24500) loss: 1.036054\n",
      "(Iteration 4101 / 24500) loss: 0.921416\n",
      "(Iteration 4121 / 24500) loss: 1.072434\n",
      "(Iteration 4141 / 24500) loss: 0.850248\n",
      "(Iteration 4161 / 24500) loss: 1.001601\n",
      "(Iteration 4181 / 24500) loss: 0.876782\n",
      "(Iteration 4201 / 24500) loss: 0.958623\n",
      "(Iteration 4221 / 24500) loss: 0.977791\n",
      "(Iteration 4241 / 24500) loss: 0.896812\n",
      "(Iteration 4261 / 24500) loss: 0.933482\n",
      "(Iteration 4281 / 24500) loss: 1.046369\n",
      "(Iteration 4301 / 24500) loss: 0.957550\n",
      "(Iteration 4321 / 24500) loss: 0.993737\n",
      "(Iteration 4341 / 24500) loss: 0.966739\n",
      "(Iteration 4361 / 24500) loss: 1.043079\n",
      "(Iteration 4381 / 24500) loss: 1.053470\n",
      "(Iteration 4401 / 24500) loss: 1.003729\n",
      "(Epoch 9 / 50) train acc: 0.661000; val_acc: 0.592000\n",
      "(Iteration 4421 / 24500) loss: 0.993753\n",
      "(Iteration 4441 / 24500) loss: 1.189854\n",
      "(Iteration 4461 / 24500) loss: 1.168811\n",
      "(Iteration 4481 / 24500) loss: 0.966892\n",
      "(Iteration 4501 / 24500) loss: 0.958867\n",
      "(Iteration 4521 / 24500) loss: 1.080684\n",
      "(Iteration 4541 / 24500) loss: 0.922666\n",
      "(Iteration 4561 / 24500) loss: 1.080140\n",
      "(Iteration 4581 / 24500) loss: 0.936738\n",
      "(Iteration 4601 / 24500) loss: 1.006580\n",
      "(Iteration 4621 / 24500) loss: 0.914078\n",
      "(Iteration 4641 / 24500) loss: 0.861758\n",
      "(Iteration 4661 / 24500) loss: 0.900741\n",
      "(Iteration 4681 / 24500) loss: 0.928122\n",
      "(Iteration 4701 / 24500) loss: 0.911756\n",
      "(Iteration 4721 / 24500) loss: 0.757334\n",
      "(Iteration 4741 / 24500) loss: 1.058546\n",
      "(Iteration 4761 / 24500) loss: 0.794507\n",
      "(Iteration 4781 / 24500) loss: 0.805860\n",
      "(Iteration 4801 / 24500) loss: 0.941150\n",
      "(Iteration 4821 / 24500) loss: 1.015297\n",
      "(Iteration 4841 / 24500) loss: 0.983471\n",
      "(Iteration 4861 / 24500) loss: 1.030016\n",
      "(Iteration 4881 / 24500) loss: 0.943060\n",
      "(Epoch 10 / 50) train acc: 0.686000; val_acc: 0.629000\n",
      "(Iteration 4901 / 24500) loss: 0.748671\n",
      "(Iteration 4921 / 24500) loss: 0.754376\n",
      "(Iteration 4941 / 24500) loss: 1.002315\n",
      "(Iteration 4961 / 24500) loss: 0.878007\n",
      "(Iteration 4981 / 24500) loss: 0.944490\n",
      "(Iteration 5001 / 24500) loss: 1.373825\n",
      "(Iteration 5021 / 24500) loss: 1.049562\n",
      "(Iteration 5041 / 24500) loss: 1.105055\n",
      "(Iteration 5061 / 24500) loss: 0.828286\n",
      "(Iteration 5081 / 24500) loss: 0.945346\n",
      "(Iteration 5101 / 24500) loss: 0.889877\n",
      "(Iteration 5121 / 24500) loss: 1.129373\n",
      "(Iteration 5141 / 24500) loss: 0.973801\n",
      "(Iteration 5161 / 24500) loss: 0.790048\n",
      "(Iteration 5181 / 24500) loss: 0.779288\n",
      "(Iteration 5201 / 24500) loss: 1.309449\n",
      "(Iteration 5221 / 24500) loss: 0.954402\n",
      "(Iteration 5241 / 24500) loss: 1.007968\n",
      "(Iteration 5261 / 24500) loss: 0.829122\n",
      "(Iteration 5281 / 24500) loss: 0.782162\n",
      "(Iteration 5301 / 24500) loss: 0.990825\n",
      "(Iteration 5321 / 24500) loss: 0.907106\n",
      "(Iteration 5341 / 24500) loss: 0.732214\n",
      "(Iteration 5361 / 24500) loss: 0.932955\n",
      "(Iteration 5381 / 24500) loss: 1.027055\n",
      "(Epoch 11 / 50) train acc: 0.734000; val_acc: 0.645000\n",
      "(Iteration 5401 / 24500) loss: 0.767386\n",
      "(Iteration 5421 / 24500) loss: 0.749055\n",
      "(Iteration 5441 / 24500) loss: 0.904132\n",
      "(Iteration 5461 / 24500) loss: 0.937805\n",
      "(Iteration 5481 / 24500) loss: 0.877184\n",
      "(Iteration 5501 / 24500) loss: 0.935513\n",
      "(Iteration 5521 / 24500) loss: 0.849620\n",
      "(Iteration 5541 / 24500) loss: 0.844541\n",
      "(Iteration 5561 / 24500) loss: 0.961094\n",
      "(Iteration 5581 / 24500) loss: 0.828415\n",
      "(Iteration 5601 / 24500) loss: 0.835977\n",
      "(Iteration 5621 / 24500) loss: 0.788791\n",
      "(Iteration 5641 / 24500) loss: 1.189279\n",
      "(Iteration 5661 / 24500) loss: 0.909151\n",
      "(Iteration 5681 / 24500) loss: 0.909166\n",
      "(Iteration 5701 / 24500) loss: 1.125500\n",
      "(Iteration 5721 / 24500) loss: 1.051165\n",
      "(Iteration 5741 / 24500) loss: 0.743793\n",
      "(Iteration 5761 / 24500) loss: 1.015807\n",
      "(Iteration 5781 / 24500) loss: 0.883250\n",
      "(Iteration 5801 / 24500) loss: 0.939940\n",
      "(Iteration 5821 / 24500) loss: 0.857302\n",
      "(Iteration 5841 / 24500) loss: 1.043902\n",
      "(Iteration 5861 / 24500) loss: 0.841044\n",
      "(Epoch 12 / 50) train acc: 0.729000; val_acc: 0.624000\n",
      "(Iteration 5881 / 24500) loss: 0.965777\n",
      "(Iteration 5901 / 24500) loss: 0.913312\n",
      "(Iteration 5921 / 24500) loss: 0.980300\n",
      "(Iteration 5941 / 24500) loss: 0.916480\n",
      "(Iteration 5961 / 24500) loss: 0.808941\n",
      "(Iteration 5981 / 24500) loss: 0.832440\n",
      "(Iteration 6001 / 24500) loss: 1.070087\n",
      "(Iteration 6021 / 24500) loss: 1.018642\n",
      "(Iteration 6041 / 24500) loss: 0.809384\n",
      "(Iteration 6061 / 24500) loss: 1.114869\n",
      "(Iteration 6081 / 24500) loss: 0.975685\n",
      "(Iteration 6101 / 24500) loss: 0.984997\n",
      "(Iteration 6121 / 24500) loss: 0.873289\n",
      "(Iteration 6141 / 24500) loss: 0.718060\n",
      "(Iteration 6161 / 24500) loss: 0.888688\n",
      "(Iteration 6181 / 24500) loss: 0.790801\n",
      "(Iteration 6201 / 24500) loss: 0.774903\n",
      "(Iteration 6221 / 24500) loss: 0.962716\n",
      "(Iteration 6241 / 24500) loss: 1.030358\n",
      "(Iteration 6261 / 24500) loss: 0.865131\n",
      "(Iteration 6281 / 24500) loss: 0.920341\n",
      "(Iteration 6301 / 24500) loss: 0.715707\n",
      "(Iteration 6321 / 24500) loss: 0.879911\n",
      "(Iteration 6341 / 24500) loss: 0.782044\n",
      "(Iteration 6361 / 24500) loss: 0.907484\n",
      "(Epoch 13 / 50) train acc: 0.737000; val_acc: 0.636000\n",
      "(Iteration 6381 / 24500) loss: 0.811567\n",
      "(Iteration 6401 / 24500) loss: 0.841719\n",
      "(Iteration 6421 / 24500) loss: 0.972775\n",
      "(Iteration 6441 / 24500) loss: 1.056347\n",
      "(Iteration 6461 / 24500) loss: 0.826783\n",
      "(Iteration 6481 / 24500) loss: 0.924314\n",
      "(Iteration 6501 / 24500) loss: 0.822958\n",
      "(Iteration 6521 / 24500) loss: 0.719240\n",
      "(Iteration 6541 / 24500) loss: 1.056098\n",
      "(Iteration 6561 / 24500) loss: 0.722792\n",
      "(Iteration 6581 / 24500) loss: 0.888123\n",
      "(Iteration 6601 / 24500) loss: 0.981499\n",
      "(Iteration 6621 / 24500) loss: 0.954512\n",
      "(Iteration 6641 / 24500) loss: 0.793533\n",
      "(Iteration 6661 / 24500) loss: 0.859533\n",
      "(Iteration 6681 / 24500) loss: 0.999806\n",
      "(Iteration 6701 / 24500) loss: 0.772053\n",
      "(Iteration 6721 / 24500) loss: 0.965230\n",
      "(Iteration 6741 / 24500) loss: 1.019451\n",
      "(Iteration 6761 / 24500) loss: 0.830556\n",
      "(Iteration 6781 / 24500) loss: 0.881641\n",
      "(Iteration 6801 / 24500) loss: 0.830882\n",
      "(Iteration 6821 / 24500) loss: 0.937473\n",
      "(Iteration 6841 / 24500) loss: 1.124667\n",
      "(Epoch 14 / 50) train acc: 0.758000; val_acc: 0.638000\n",
      "(Iteration 6861 / 24500) loss: 0.917426\n",
      "(Iteration 6881 / 24500) loss: 0.828163\n",
      "(Iteration 6901 / 24500) loss: 1.273699\n",
      "(Iteration 6921 / 24500) loss: 0.870611\n",
      "(Iteration 6941 / 24500) loss: 0.925527\n",
      "(Iteration 6961 / 24500) loss: 0.887899\n",
      "(Iteration 6981 / 24500) loss: 0.738212\n",
      "(Iteration 7001 / 24500) loss: 0.817036\n",
      "(Iteration 7021 / 24500) loss: 0.868666\n",
      "(Iteration 7041 / 24500) loss: 0.792369\n",
      "(Iteration 7061 / 24500) loss: 0.590955\n",
      "(Iteration 7081 / 24500) loss: 0.864987\n",
      "(Iteration 7101 / 24500) loss: 0.992617\n",
      "(Iteration 7121 / 24500) loss: 0.773061\n",
      "(Iteration 7141 / 24500) loss: 0.732198\n",
      "(Iteration 7161 / 24500) loss: 0.906718\n",
      "(Iteration 7181 / 24500) loss: 0.848904\n",
      "(Iteration 7201 / 24500) loss: 0.949865\n",
      "(Iteration 7221 / 24500) loss: 0.988787\n",
      "(Iteration 7241 / 24500) loss: 0.906554\n",
      "(Iteration 7261 / 24500) loss: 0.784147\n",
      "(Iteration 7281 / 24500) loss: 1.058420\n",
      "(Iteration 7301 / 24500) loss: 0.940064\n",
      "(Iteration 7321 / 24500) loss: 0.856413\n",
      "(Iteration 7341 / 24500) loss: 0.612836\n",
      "(Epoch 15 / 50) train acc: 0.721000; val_acc: 0.622000\n",
      "(Iteration 7361 / 24500) loss: 0.780389\n",
      "(Iteration 7381 / 24500) loss: 0.743844\n",
      "(Iteration 7401 / 24500) loss: 0.903836\n",
      "(Iteration 7421 / 24500) loss: 1.062305\n",
      "(Iteration 7441 / 24500) loss: 0.782368\n",
      "(Iteration 7461 / 24500) loss: 0.524972\n",
      "(Iteration 7481 / 24500) loss: 0.832268\n",
      "(Iteration 7501 / 24500) loss: 0.869746\n",
      "(Iteration 7521 / 24500) loss: 0.776056\n",
      "(Iteration 7541 / 24500) loss: 0.918118\n",
      "(Iteration 7561 / 24500) loss: 0.772061\n",
      "(Iteration 7581 / 24500) loss: 0.908395\n",
      "(Iteration 7601 / 24500) loss: 0.804181\n",
      "(Iteration 7621 / 24500) loss: 0.824084\n",
      "(Iteration 7641 / 24500) loss: 0.913709\n",
      "(Iteration 7661 / 24500) loss: 0.618177\n",
      "(Iteration 7681 / 24500) loss: 0.655545\n",
      "(Iteration 7701 / 24500) loss: 0.723993\n",
      "(Iteration 7721 / 24500) loss: 0.842370\n",
      "(Iteration 7741 / 24500) loss: 0.781929\n",
      "(Iteration 7761 / 24500) loss: 0.917086\n",
      "(Iteration 7781 / 24500) loss: 0.711756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7801 / 24500) loss: 0.813424\n",
      "(Iteration 7821 / 24500) loss: 0.716891\n",
      "(Epoch 16 / 50) train acc: 0.741000; val_acc: 0.642000\n",
      "(Iteration 7841 / 24500) loss: 0.779785\n",
      "(Iteration 7861 / 24500) loss: 0.741204\n",
      "(Iteration 7881 / 24500) loss: 0.759240\n",
      "(Iteration 7901 / 24500) loss: 0.900765\n",
      "(Iteration 7921 / 24500) loss: 0.743585\n",
      "(Iteration 7941 / 24500) loss: 0.854323\n",
      "(Iteration 7961 / 24500) loss: 0.999803\n",
      "(Iteration 7981 / 24500) loss: 0.823973\n",
      "(Iteration 8001 / 24500) loss: 0.789538\n",
      "(Iteration 8021 / 24500) loss: 0.955183\n",
      "(Iteration 8041 / 24500) loss: 0.708653\n",
      "(Iteration 8061 / 24500) loss: 0.874199\n",
      "(Iteration 8081 / 24500) loss: 0.804018\n",
      "(Iteration 8101 / 24500) loss: 0.743633\n",
      "(Iteration 8121 / 24500) loss: 1.019109\n",
      "(Iteration 8141 / 24500) loss: 0.817373\n",
      "(Iteration 8161 / 24500) loss: 0.803586\n",
      "(Iteration 8181 / 24500) loss: 0.821317\n",
      "(Iteration 8201 / 24500) loss: 0.866174\n",
      "(Iteration 8221 / 24500) loss: 0.997116\n",
      "(Iteration 8241 / 24500) loss: 0.753641\n",
      "(Iteration 8261 / 24500) loss: 0.758792\n",
      "(Iteration 8281 / 24500) loss: 0.769279\n",
      "(Iteration 8301 / 24500) loss: 1.039221\n",
      "(Iteration 8321 / 24500) loss: 0.888475\n",
      "(Epoch 17 / 50) train acc: 0.752000; val_acc: 0.637000\n",
      "(Iteration 8341 / 24500) loss: 0.883827\n",
      "(Iteration 8361 / 24500) loss: 0.638066\n",
      "(Iteration 8381 / 24500) loss: 0.779811\n",
      "(Iteration 8401 / 24500) loss: 0.685209\n",
      "(Iteration 8421 / 24500) loss: 0.901931\n",
      "(Iteration 8441 / 24500) loss: 0.870773\n",
      "(Iteration 8461 / 24500) loss: 0.810213\n",
      "(Iteration 8481 / 24500) loss: 1.029153\n",
      "(Iteration 8501 / 24500) loss: 0.638048\n",
      "(Iteration 8521 / 24500) loss: 0.850114\n",
      "(Iteration 8541 / 24500) loss: 0.792232\n",
      "(Iteration 8561 / 24500) loss: 0.819323\n",
      "(Iteration 8581 / 24500) loss: 0.812766\n",
      "(Iteration 8601 / 24500) loss: 0.616975\n",
      "(Iteration 8621 / 24500) loss: 1.125629\n",
      "(Iteration 8641 / 24500) loss: 0.726390\n",
      "(Iteration 8661 / 24500) loss: 0.778732\n",
      "(Iteration 8681 / 24500) loss: 0.889817\n",
      "(Iteration 8701 / 24500) loss: 0.737898\n",
      "(Iteration 8721 / 24500) loss: 0.676269\n",
      "(Iteration 8741 / 24500) loss: 0.673487\n",
      "(Iteration 8761 / 24500) loss: 0.767269\n",
      "(Iteration 8781 / 24500) loss: 0.681259\n",
      "(Iteration 8801 / 24500) loss: 0.891502\n",
      "(Epoch 18 / 50) train acc: 0.767000; val_acc: 0.659000\n",
      "(Iteration 8821 / 24500) loss: 0.947200\n",
      "(Iteration 8841 / 24500) loss: 0.704811\n",
      "(Iteration 8861 / 24500) loss: 0.932530\n",
      "(Iteration 8881 / 24500) loss: 0.609200\n",
      "(Iteration 8901 / 24500) loss: 0.959397\n",
      "(Iteration 8921 / 24500) loss: 0.594190\n",
      "(Iteration 8941 / 24500) loss: 1.078085\n",
      "(Iteration 8961 / 24500) loss: 0.857199\n",
      "(Iteration 8981 / 24500) loss: 0.598467\n",
      "(Iteration 9001 / 24500) loss: 0.612530\n",
      "(Iteration 9021 / 24500) loss: 0.723452\n",
      "(Iteration 9041 / 24500) loss: 0.834765\n",
      "(Iteration 9061 / 24500) loss: 0.708079\n",
      "(Iteration 9081 / 24500) loss: 0.901373\n",
      "(Iteration 9101 / 24500) loss: 0.737463\n",
      "(Iteration 9121 / 24500) loss: 0.737128\n",
      "(Iteration 9141 / 24500) loss: 0.673192\n",
      "(Iteration 9161 / 24500) loss: 0.731067\n",
      "(Iteration 9181 / 24500) loss: 0.913751\n",
      "(Iteration 9201 / 24500) loss: 0.772180\n",
      "(Iteration 9221 / 24500) loss: 0.708929\n",
      "(Iteration 9241 / 24500) loss: 0.711615\n",
      "(Iteration 9261 / 24500) loss: 0.721774\n",
      "(Iteration 9281 / 24500) loss: 0.711688\n",
      "(Iteration 9301 / 24500) loss: 0.895482\n",
      "(Epoch 19 / 50) train acc: 0.777000; val_acc: 0.642000\n",
      "(Iteration 9321 / 24500) loss: 0.568421\n",
      "(Iteration 9341 / 24500) loss: 0.772563\n",
      "(Iteration 9361 / 24500) loss: 0.657802\n",
      "(Iteration 9381 / 24500) loss: 0.777409\n",
      "(Iteration 9401 / 24500) loss: 0.737802\n",
      "(Iteration 9421 / 24500) loss: 0.765309\n",
      "(Iteration 9441 / 24500) loss: 0.559231\n",
      "(Iteration 9461 / 24500) loss: 0.806140\n",
      "(Iteration 9481 / 24500) loss: 0.913871\n",
      "(Iteration 9501 / 24500) loss: 0.533223\n",
      "(Iteration 9521 / 24500) loss: 0.731872\n",
      "(Iteration 9541 / 24500) loss: 0.848605\n",
      "(Iteration 9561 / 24500) loss: 1.000092\n",
      "(Iteration 9581 / 24500) loss: 0.965767\n",
      "(Iteration 9601 / 24500) loss: 0.731255\n",
      "(Iteration 9621 / 24500) loss: 0.767607\n",
      "(Iteration 9641 / 24500) loss: 0.697491\n",
      "(Iteration 9661 / 24500) loss: 0.661343\n",
      "(Iteration 9681 / 24500) loss: 0.700328\n",
      "(Iteration 9701 / 24500) loss: 0.792534\n",
      "(Iteration 9721 / 24500) loss: 0.711005\n",
      "(Iteration 9741 / 24500) loss: 0.803096\n",
      "(Iteration 9761 / 24500) loss: 0.543194\n",
      "(Iteration 9781 / 24500) loss: 0.688945\n",
      "(Epoch 20 / 50) train acc: 0.786000; val_acc: 0.654000\n",
      "(Iteration 9801 / 24500) loss: 0.703014\n",
      "(Iteration 9821 / 24500) loss: 0.849959\n",
      "(Iteration 9841 / 24500) loss: 0.692367\n",
      "(Iteration 9861 / 24500) loss: 0.653449\n",
      "(Iteration 9881 / 24500) loss: 0.719644\n",
      "(Iteration 9901 / 24500) loss: 0.675023\n",
      "(Iteration 9921 / 24500) loss: 0.660919\n",
      "(Iteration 9941 / 24500) loss: 0.749646\n",
      "(Iteration 9961 / 24500) loss: 0.818601\n",
      "(Iteration 9981 / 24500) loss: 0.659424\n",
      "(Iteration 10001 / 24500) loss: 0.961903\n",
      "(Iteration 10021 / 24500) loss: 0.822042\n",
      "(Iteration 10041 / 24500) loss: 0.850603\n",
      "(Iteration 10061 / 24500) loss: 0.825344\n",
      "(Iteration 10081 / 24500) loss: 0.606812\n",
      "(Iteration 10101 / 24500) loss: 0.649774\n",
      "(Iteration 10121 / 24500) loss: 0.595900\n",
      "(Iteration 10141 / 24500) loss: 0.707141\n",
      "(Iteration 10161 / 24500) loss: 0.735540\n",
      "(Iteration 10181 / 24500) loss: 0.616017\n",
      "(Iteration 10201 / 24500) loss: 0.887615\n",
      "(Iteration 10221 / 24500) loss: 0.761628\n",
      "(Iteration 10241 / 24500) loss: 0.588950\n",
      "(Iteration 10261 / 24500) loss: 0.783045\n",
      "(Iteration 10281 / 24500) loss: 0.684294\n",
      "(Epoch 21 / 50) train acc: 0.792000; val_acc: 0.653000\n",
      "(Iteration 10301 / 24500) loss: 0.598383\n",
      "(Iteration 10321 / 24500) loss: 0.787118\n",
      "(Iteration 10341 / 24500) loss: 0.729478\n",
      "(Iteration 10361 / 24500) loss: 0.836371\n",
      "(Iteration 10381 / 24500) loss: 0.722071\n",
      "(Iteration 10401 / 24500) loss: 0.813553\n",
      "(Iteration 10421 / 24500) loss: 0.656525\n",
      "(Iteration 10441 / 24500) loss: 0.880818\n",
      "(Iteration 10461 / 24500) loss: 0.846957\n",
      "(Iteration 10481 / 24500) loss: 0.857321\n",
      "(Iteration 10501 / 24500) loss: 0.747204\n",
      "(Iteration 10521 / 24500) loss: 0.838187\n",
      "(Iteration 10541 / 24500) loss: 0.718127\n",
      "(Iteration 10561 / 24500) loss: 0.628975\n",
      "(Iteration 10581 / 24500) loss: 0.929070\n",
      "(Iteration 10601 / 24500) loss: 0.865738\n",
      "(Iteration 10621 / 24500) loss: 0.617962\n",
      "(Iteration 10641 / 24500) loss: 0.758688\n",
      "(Iteration 10661 / 24500) loss: 0.689382\n",
      "(Iteration 10681 / 24500) loss: 0.666339\n",
      "(Iteration 10701 / 24500) loss: 0.820803\n",
      "(Iteration 10721 / 24500) loss: 0.765699\n",
      "(Iteration 10741 / 24500) loss: 0.808035\n",
      "(Iteration 10761 / 24500) loss: 0.731543\n",
      "(Epoch 22 / 50) train acc: 0.796000; val_acc: 0.657000\n",
      "(Iteration 10781 / 24500) loss: 0.651140\n",
      "(Iteration 10801 / 24500) loss: 0.716545\n",
      "(Iteration 10821 / 24500) loss: 0.608051\n",
      "(Iteration 10841 / 24500) loss: 0.744341\n",
      "(Iteration 10861 / 24500) loss: 0.767491\n",
      "(Iteration 10881 / 24500) loss: 0.647995\n",
      "(Iteration 10901 / 24500) loss: 0.697575\n",
      "(Iteration 10921 / 24500) loss: 0.851053\n",
      "(Iteration 10941 / 24500) loss: 0.785236\n",
      "(Iteration 10961 / 24500) loss: 0.639696\n",
      "(Iteration 10981 / 24500) loss: 0.889346\n",
      "(Iteration 11001 / 24500) loss: 0.714794\n",
      "(Iteration 11021 / 24500) loss: 0.918185\n",
      "(Iteration 11041 / 24500) loss: 0.788359\n",
      "(Iteration 11061 / 24500) loss: 0.815697\n",
      "(Iteration 11081 / 24500) loss: 0.841954\n",
      "(Iteration 11101 / 24500) loss: 0.841825\n",
      "(Iteration 11121 / 24500) loss: 0.718335\n",
      "(Iteration 11141 / 24500) loss: 0.599368\n",
      "(Iteration 11161 / 24500) loss: 0.823152\n",
      "(Iteration 11181 / 24500) loss: 0.690615\n",
      "(Iteration 11201 / 24500) loss: 0.707262\n",
      "(Iteration 11221 / 24500) loss: 0.668995\n",
      "(Iteration 11241 / 24500) loss: 0.526991\n",
      "(Iteration 11261 / 24500) loss: 0.588451\n",
      "(Epoch 23 / 50) train acc: 0.781000; val_acc: 0.650000\n",
      "(Iteration 11281 / 24500) loss: 0.697254\n",
      "(Iteration 11301 / 24500) loss: 0.718875\n",
      "(Iteration 11321 / 24500) loss: 0.755299\n",
      "(Iteration 11341 / 24500) loss: 0.584246\n",
      "(Iteration 11361 / 24500) loss: 0.773344\n",
      "(Iteration 11381 / 24500) loss: 0.686364\n",
      "(Iteration 11401 / 24500) loss: 0.817195\n",
      "(Iteration 11421 / 24500) loss: 0.776207\n",
      "(Iteration 11441 / 24500) loss: 0.509947\n",
      "(Iteration 11461 / 24500) loss: 0.626480\n",
      "(Iteration 11481 / 24500) loss: 0.776251\n",
      "(Iteration 11501 / 24500) loss: 0.678686\n",
      "(Iteration 11521 / 24500) loss: 0.653008\n",
      "(Iteration 11541 / 24500) loss: 0.853338\n",
      "(Iteration 11561 / 24500) loss: 0.968842\n",
      "(Iteration 11581 / 24500) loss: 0.701150\n",
      "(Iteration 11601 / 24500) loss: 0.808871\n",
      "(Iteration 11621 / 24500) loss: 0.613298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11641 / 24500) loss: 0.722071\n",
      "(Iteration 11661 / 24500) loss: 0.630687\n",
      "(Iteration 11681 / 24500) loss: 0.738591\n",
      "(Iteration 11701 / 24500) loss: 0.791418\n",
      "(Iteration 11721 / 24500) loss: 0.790184\n",
      "(Iteration 11741 / 24500) loss: 0.879826\n",
      "(Epoch 24 / 50) train acc: 0.792000; val_acc: 0.656000\n",
      "(Iteration 11761 / 24500) loss: 0.652116\n",
      "(Iteration 11781 / 24500) loss: 0.674069\n",
      "(Iteration 11801 / 24500) loss: 0.867734\n",
      "(Iteration 11821 / 24500) loss: 0.721629\n",
      "(Iteration 11841 / 24500) loss: 0.803480\n",
      "(Iteration 11861 / 24500) loss: 0.633440\n",
      "(Iteration 11881 / 24500) loss: 0.882273\n",
      "(Iteration 11901 / 24500) loss: 0.690983\n",
      "(Iteration 11921 / 24500) loss: 0.576287\n",
      "(Iteration 11941 / 24500) loss: 0.729249\n",
      "(Iteration 11961 / 24500) loss: 0.688203\n",
      "(Iteration 11981 / 24500) loss: 0.605108\n",
      "(Iteration 12001 / 24500) loss: 0.689615\n",
      "(Iteration 12021 / 24500) loss: 0.652012\n",
      "(Iteration 12041 / 24500) loss: 0.716590\n",
      "(Iteration 12061 / 24500) loss: 0.690717\n",
      "(Iteration 12081 / 24500) loss: 0.627646\n",
      "(Iteration 12101 / 24500) loss: 0.624981\n",
      "(Iteration 12121 / 24500) loss: 0.604877\n",
      "(Iteration 12141 / 24500) loss: 0.671153\n",
      "(Iteration 12161 / 24500) loss: 0.799778\n",
      "(Iteration 12181 / 24500) loss: 0.633593\n",
      "(Iteration 12201 / 24500) loss: 0.683867\n",
      "(Iteration 12221 / 24500) loss: 0.546922\n",
      "(Iteration 12241 / 24500) loss: 0.702078\n",
      "(Epoch 25 / 50) train acc: 0.810000; val_acc: 0.653000\n",
      "(Iteration 12261 / 24500) loss: 0.817803\n",
      "(Iteration 12281 / 24500) loss: 0.647412\n",
      "(Iteration 12301 / 24500) loss: 0.889055\n",
      "(Iteration 12321 / 24500) loss: 0.540963\n",
      "(Iteration 12341 / 24500) loss: 0.731691\n",
      "(Iteration 12361 / 24500) loss: 0.528237\n",
      "(Iteration 12381 / 24500) loss: 0.649899\n",
      "(Iteration 12401 / 24500) loss: 0.684453\n",
      "(Iteration 12421 / 24500) loss: 0.704621\n",
      "(Iteration 12441 / 24500) loss: 0.515548\n",
      "(Iteration 12461 / 24500) loss: 0.671446\n",
      "(Iteration 12481 / 24500) loss: 0.759567\n",
      "(Iteration 12501 / 24500) loss: 0.678642\n",
      "(Iteration 12521 / 24500) loss: 0.767444\n",
      "(Iteration 12541 / 24500) loss: 0.757355\n",
      "(Iteration 12561 / 24500) loss: 0.617739\n",
      "(Iteration 12581 / 24500) loss: 0.766978\n",
      "(Iteration 12601 / 24500) loss: 0.815745\n",
      "(Iteration 12621 / 24500) loss: 0.548883\n",
      "(Iteration 12641 / 24500) loss: 0.657354\n",
      "(Iteration 12661 / 24500) loss: 0.456090\n",
      "(Iteration 12681 / 24500) loss: 0.638643\n",
      "(Iteration 12701 / 24500) loss: 0.589374\n",
      "(Iteration 12721 / 24500) loss: 0.632227\n",
      "(Epoch 26 / 50) train acc: 0.821000; val_acc: 0.664000\n",
      "(Iteration 12741 / 24500) loss: 0.829243\n",
      "(Iteration 12761 / 24500) loss: 0.508841\n",
      "(Iteration 12781 / 24500) loss: 0.719791\n",
      "(Iteration 12801 / 24500) loss: 0.648951\n",
      "(Iteration 12821 / 24500) loss: 0.759163\n",
      "(Iteration 12841 / 24500) loss: 0.591871\n",
      "(Iteration 12861 / 24500) loss: 0.602912\n",
      "(Iteration 12881 / 24500) loss: 0.568044\n",
      "(Iteration 12901 / 24500) loss: 0.755028\n",
      "(Iteration 12921 / 24500) loss: 0.921387\n",
      "(Iteration 12941 / 24500) loss: 0.625417\n",
      "(Iteration 12961 / 24500) loss: 0.722130\n",
      "(Iteration 12981 / 24500) loss: 0.657448\n",
      "(Iteration 13001 / 24500) loss: 0.805250\n",
      "(Iteration 13021 / 24500) loss: 0.622079\n",
      "(Iteration 13041 / 24500) loss: 0.580098\n",
      "(Iteration 13061 / 24500) loss: 0.660351\n",
      "(Iteration 13081 / 24500) loss: 0.776424\n",
      "(Iteration 13101 / 24500) loss: 0.628405\n",
      "(Iteration 13121 / 24500) loss: 0.624925\n",
      "(Iteration 13141 / 24500) loss: 0.585176\n",
      "(Iteration 13161 / 24500) loss: 0.530141\n",
      "(Iteration 13181 / 24500) loss: 0.610755\n",
      "(Iteration 13201 / 24500) loss: 0.762241\n",
      "(Iteration 13221 / 24500) loss: 0.646147\n",
      "(Epoch 27 / 50) train acc: 0.814000; val_acc: 0.664000\n",
      "(Iteration 13241 / 24500) loss: 0.536975\n",
      "(Iteration 13261 / 24500) loss: 0.557452\n",
      "(Iteration 13281 / 24500) loss: 0.617626\n",
      "(Iteration 13301 / 24500) loss: 0.515658\n",
      "(Iteration 13321 / 24500) loss: 0.527551\n",
      "(Iteration 13341 / 24500) loss: 0.502348\n",
      "(Iteration 13361 / 24500) loss: 0.551932\n",
      "(Iteration 13381 / 24500) loss: 0.489281\n",
      "(Iteration 13401 / 24500) loss: 0.631644\n",
      "(Iteration 13421 / 24500) loss: 0.534868\n",
      "(Iteration 13441 / 24500) loss: 0.584510\n",
      "(Iteration 13461 / 24500) loss: 0.647069\n",
      "(Iteration 13481 / 24500) loss: 0.455418\n",
      "(Iteration 13501 / 24500) loss: 0.781387\n",
      "(Iteration 13521 / 24500) loss: 0.538779\n",
      "(Iteration 13541 / 24500) loss: 0.766222\n",
      "(Iteration 13561 / 24500) loss: 0.684126\n",
      "(Iteration 13581 / 24500) loss: 0.682461\n",
      "(Iteration 13601 / 24500) loss: 0.513432\n",
      "(Iteration 13621 / 24500) loss: 0.628899\n",
      "(Iteration 13641 / 24500) loss: 0.770398\n",
      "(Iteration 13661 / 24500) loss: 0.514942\n",
      "(Iteration 13681 / 24500) loss: 0.527647\n",
      "(Iteration 13701 / 24500) loss: 0.583566\n",
      "(Epoch 28 / 50) train acc: 0.805000; val_acc: 0.631000\n",
      "(Iteration 13721 / 24500) loss: 0.553475\n",
      "(Iteration 13741 / 24500) loss: 0.665273\n",
      "(Iteration 13761 / 24500) loss: 0.513030\n",
      "(Iteration 13781 / 24500) loss: 0.493663\n",
      "(Iteration 13801 / 24500) loss: 0.601976\n",
      "(Iteration 13821 / 24500) loss: 0.560163\n",
      "(Iteration 13841 / 24500) loss: 0.628640\n",
      "(Iteration 13861 / 24500) loss: 0.771288\n",
      "(Iteration 13881 / 24500) loss: 0.782892\n",
      "(Iteration 13901 / 24500) loss: 0.608669\n",
      "(Iteration 13921 / 24500) loss: 0.676342\n",
      "(Iteration 13941 / 24500) loss: 0.678747\n",
      "(Iteration 13961 / 24500) loss: 0.526037\n",
      "(Iteration 13981 / 24500) loss: 0.754294\n",
      "(Iteration 14001 / 24500) loss: 0.630125\n",
      "(Iteration 14021 / 24500) loss: 0.684597\n",
      "(Iteration 14041 / 24500) loss: 0.578941\n",
      "(Iteration 14061 / 24500) loss: 0.581704\n",
      "(Iteration 14081 / 24500) loss: 0.394402\n",
      "(Iteration 14101 / 24500) loss: 0.719641\n",
      "(Iteration 14121 / 24500) loss: 0.554431\n",
      "(Iteration 14141 / 24500) loss: 0.656468\n",
      "(Iteration 14161 / 24500) loss: 0.576947\n",
      "(Iteration 14181 / 24500) loss: 0.606167\n",
      "(Iteration 14201 / 24500) loss: 0.693486\n",
      "(Epoch 29 / 50) train acc: 0.811000; val_acc: 0.650000\n",
      "(Iteration 14221 / 24500) loss: 0.860476\n",
      "(Iteration 14241 / 24500) loss: 0.620848\n",
      "(Iteration 14261 / 24500) loss: 0.576225\n",
      "(Iteration 14281 / 24500) loss: 0.521162\n",
      "(Iteration 14301 / 24500) loss: 0.544258\n",
      "(Iteration 14321 / 24500) loss: 0.629412\n",
      "(Iteration 14341 / 24500) loss: 0.501347\n",
      "(Iteration 14361 / 24500) loss: 0.763096\n",
      "(Iteration 14381 / 24500) loss: 0.454138\n",
      "(Iteration 14401 / 24500) loss: 0.785373\n",
      "(Iteration 14421 / 24500) loss: 0.580236\n",
      "(Iteration 14441 / 24500) loss: 0.617689\n",
      "(Iteration 14461 / 24500) loss: 0.540600\n",
      "(Iteration 14481 / 24500) loss: 0.561558\n",
      "(Iteration 14501 / 24500) loss: 0.493663\n",
      "(Iteration 14521 / 24500) loss: 0.519938\n",
      "(Iteration 14541 / 24500) loss: 0.477464\n",
      "(Iteration 14561 / 24500) loss: 0.686985\n",
      "(Iteration 14581 / 24500) loss: 0.715784\n",
      "(Iteration 14601 / 24500) loss: 0.656937\n",
      "(Iteration 14621 / 24500) loss: 0.461632\n",
      "(Iteration 14641 / 24500) loss: 0.607533\n",
      "(Iteration 14661 / 24500) loss: 0.497903\n",
      "(Iteration 14681 / 24500) loss: 0.434808\n",
      "(Epoch 30 / 50) train acc: 0.856000; val_acc: 0.664000\n",
      "(Iteration 14701 / 24500) loss: 0.524608\n",
      "(Iteration 14721 / 24500) loss: 0.595610\n",
      "(Iteration 14741 / 24500) loss: 0.739097\n",
      "(Iteration 14761 / 24500) loss: 0.712467\n",
      "(Iteration 14781 / 24500) loss: 0.469135\n",
      "(Iteration 14801 / 24500) loss: 0.461325\n",
      "(Iteration 14821 / 24500) loss: 0.495992\n",
      "(Iteration 14841 / 24500) loss: 0.485277\n",
      "(Iteration 14861 / 24500) loss: 0.656926\n",
      "(Iteration 14881 / 24500) loss: 0.547983\n",
      "(Iteration 14901 / 24500) loss: 0.467063\n",
      "(Iteration 14921 / 24500) loss: 0.529722\n",
      "(Iteration 14941 / 24500) loss: 0.601700\n",
      "(Iteration 14961 / 24500) loss: 0.444561\n",
      "(Iteration 14981 / 24500) loss: 0.643506\n",
      "(Iteration 15001 / 24500) loss: 0.414336\n",
      "(Iteration 15021 / 24500) loss: 0.557934\n",
      "(Iteration 15041 / 24500) loss: 0.405929\n",
      "(Iteration 15061 / 24500) loss: 0.571726\n",
      "(Iteration 15081 / 24500) loss: 0.453872\n",
      "(Iteration 15101 / 24500) loss: 0.654709\n",
      "(Iteration 15121 / 24500) loss: 0.548383\n",
      "(Iteration 15141 / 24500) loss: 0.453450\n",
      "(Iteration 15161 / 24500) loss: 0.597889\n",
      "(Iteration 15181 / 24500) loss: 0.648569\n",
      "(Epoch 31 / 50) train acc: 0.836000; val_acc: 0.640000\n",
      "(Iteration 15201 / 24500) loss: 0.592585\n",
      "(Iteration 15221 / 24500) loss: 0.561022\n",
      "(Iteration 15241 / 24500) loss: 0.697036\n",
      "(Iteration 15261 / 24500) loss: 0.794862\n",
      "(Iteration 15281 / 24500) loss: 0.593083\n",
      "(Iteration 15301 / 24500) loss: 0.775155\n",
      "(Iteration 15321 / 24500) loss: 0.832920\n",
      "(Iteration 15341 / 24500) loss: 0.596350\n",
      "(Iteration 15361 / 24500) loss: 0.658313\n",
      "(Iteration 15381 / 24500) loss: 0.583082\n",
      "(Iteration 15401 / 24500) loss: 0.600560\n",
      "(Iteration 15421 / 24500) loss: 0.546306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 15441 / 24500) loss: 0.564583\n",
      "(Iteration 15461 / 24500) loss: 0.567188\n",
      "(Iteration 15481 / 24500) loss: 0.671955\n",
      "(Iteration 15501 / 24500) loss: 0.463050\n",
      "(Iteration 15521 / 24500) loss: 0.606714\n",
      "(Iteration 15541 / 24500) loss: 0.719962\n",
      "(Iteration 15561 / 24500) loss: 0.711386\n",
      "(Iteration 15581 / 24500) loss: 0.611058\n",
      "(Iteration 15601 / 24500) loss: 0.515171\n",
      "(Iteration 15621 / 24500) loss: 0.692849\n",
      "(Iteration 15641 / 24500) loss: 0.693162\n",
      "(Iteration 15661 / 24500) loss: 0.566349\n",
      "(Epoch 32 / 50) train acc: 0.843000; val_acc: 0.654000\n",
      "(Iteration 15681 / 24500) loss: 0.597098\n",
      "(Iteration 15701 / 24500) loss: 0.578276\n",
      "(Iteration 15721 / 24500) loss: 0.460063\n",
      "(Iteration 15741 / 24500) loss: 0.375301\n",
      "(Iteration 15761 / 24500) loss: 0.467473\n",
      "(Iteration 15781 / 24500) loss: 0.839675\n",
      "(Iteration 15801 / 24500) loss: 0.585271\n",
      "(Iteration 15821 / 24500) loss: 0.479516\n",
      "(Iteration 15841 / 24500) loss: 0.749612\n",
      "(Iteration 15861 / 24500) loss: 0.771519\n",
      "(Iteration 15881 / 24500) loss: 0.588756\n",
      "(Iteration 15901 / 24500) loss: 0.645363\n",
      "(Iteration 15921 / 24500) loss: 0.455206\n",
      "(Iteration 15941 / 24500) loss: 0.498133\n",
      "(Iteration 15961 / 24500) loss: 0.592288\n",
      "(Iteration 15981 / 24500) loss: 0.600144\n",
      "(Iteration 16001 / 24500) loss: 0.516668\n",
      "(Iteration 16021 / 24500) loss: 0.619848\n",
      "(Iteration 16041 / 24500) loss: 0.562479\n",
      "(Iteration 16061 / 24500) loss: 0.495578\n",
      "(Iteration 16081 / 24500) loss: 0.513128\n",
      "(Iteration 16101 / 24500) loss: 0.649967\n",
      "(Iteration 16121 / 24500) loss: 0.551320\n",
      "(Iteration 16141 / 24500) loss: 0.511449\n",
      "(Iteration 16161 / 24500) loss: 0.493012\n",
      "(Epoch 33 / 50) train acc: 0.846000; val_acc: 0.640000\n",
      "(Iteration 16181 / 24500) loss: 0.740378\n",
      "(Iteration 16201 / 24500) loss: 0.559551\n",
      "(Iteration 16221 / 24500) loss: 0.591436\n",
      "(Iteration 16241 / 24500) loss: 0.565456\n",
      "(Iteration 16261 / 24500) loss: 0.493958\n",
      "(Iteration 16281 / 24500) loss: 0.676870\n",
      "(Iteration 16301 / 24500) loss: 0.537153\n",
      "(Iteration 16321 / 24500) loss: 0.469511\n",
      "(Iteration 16341 / 24500) loss: 0.574207\n",
      "(Iteration 16361 / 24500) loss: 0.540318\n",
      "(Iteration 16381 / 24500) loss: 0.428251\n",
      "(Iteration 16401 / 24500) loss: 0.405965\n",
      "(Iteration 16421 / 24500) loss: 0.602159\n",
      "(Iteration 16441 / 24500) loss: 0.601605\n",
      "(Iteration 16461 / 24500) loss: 0.568501\n",
      "(Iteration 16481 / 24500) loss: 0.457088\n",
      "(Iteration 16501 / 24500) loss: 0.673938\n",
      "(Iteration 16521 / 24500) loss: 0.508780\n",
      "(Iteration 16541 / 24500) loss: 0.487194\n",
      "(Iteration 16561 / 24500) loss: 0.384277\n",
      "(Iteration 16581 / 24500) loss: 0.509433\n",
      "(Iteration 16601 / 24500) loss: 0.374873\n",
      "(Iteration 16621 / 24500) loss: 0.342888\n",
      "(Iteration 16641 / 24500) loss: 0.495691\n",
      "(Epoch 34 / 50) train acc: 0.848000; val_acc: 0.652000\n",
      "(Iteration 16661 / 24500) loss: 0.552893\n",
      "(Iteration 16681 / 24500) loss: 0.581149\n",
      "(Iteration 16701 / 24500) loss: 0.432654\n",
      "(Iteration 16721 / 24500) loss: 0.530539\n",
      "(Iteration 16741 / 24500) loss: 0.480054\n",
      "(Iteration 16761 / 24500) loss: 0.574319\n",
      "(Iteration 16781 / 24500) loss: 0.522393\n",
      "(Iteration 16801 / 24500) loss: 0.708974\n",
      "(Iteration 16821 / 24500) loss: 0.627645\n",
      "(Iteration 16841 / 24500) loss: 0.578226\n",
      "(Iteration 16861 / 24500) loss: 0.599118\n",
      "(Iteration 16881 / 24500) loss: 0.510133\n",
      "(Iteration 16901 / 24500) loss: 0.499346\n",
      "(Iteration 16921 / 24500) loss: 0.505022\n",
      "(Iteration 16941 / 24500) loss: 0.601800\n",
      "(Iteration 16961 / 24500) loss: 0.503910\n",
      "(Iteration 16981 / 24500) loss: 0.422271\n",
      "(Iteration 17001 / 24500) loss: 0.387778\n",
      "(Iteration 17021 / 24500) loss: 0.620220\n",
      "(Iteration 17041 / 24500) loss: 0.491120\n",
      "(Iteration 17061 / 24500) loss: 0.542719\n",
      "(Iteration 17081 / 24500) loss: 0.405499\n",
      "(Iteration 17101 / 24500) loss: 0.696544\n",
      "(Iteration 17121 / 24500) loss: 0.521442\n",
      "(Iteration 17141 / 24500) loss: 0.399142\n",
      "(Epoch 35 / 50) train acc: 0.854000; val_acc: 0.656000\n",
      "(Iteration 17161 / 24500) loss: 0.381868\n",
      "(Iteration 17181 / 24500) loss: 0.535951\n",
      "(Iteration 17201 / 24500) loss: 0.424992\n",
      "(Iteration 17221 / 24500) loss: 0.493819\n",
      "(Iteration 17241 / 24500) loss: 0.500265\n",
      "(Iteration 17261 / 24500) loss: 0.590386\n",
      "(Iteration 17281 / 24500) loss: 0.517667\n",
      "(Iteration 17301 / 24500) loss: 0.442488\n",
      "(Iteration 17321 / 24500) loss: 0.574560\n",
      "(Iteration 17341 / 24500) loss: 0.371478\n",
      "(Iteration 17361 / 24500) loss: 0.513567\n",
      "(Iteration 17381 / 24500) loss: 0.493089\n",
      "(Iteration 17401 / 24500) loss: 0.502483\n",
      "(Iteration 17421 / 24500) loss: 0.507847\n",
      "(Iteration 17441 / 24500) loss: 0.539082\n",
      "(Iteration 17461 / 24500) loss: 0.464242\n",
      "(Iteration 17481 / 24500) loss: 0.347496\n",
      "(Iteration 17501 / 24500) loss: 0.445585\n",
      "(Iteration 17521 / 24500) loss: 0.608969\n",
      "(Iteration 17541 / 24500) loss: 0.386879\n",
      "(Iteration 17561 / 24500) loss: 0.538013\n",
      "(Iteration 17581 / 24500) loss: 0.539244\n",
      "(Iteration 17601 / 24500) loss: 0.494302\n",
      "(Iteration 17621 / 24500) loss: 0.524639\n",
      "(Epoch 36 / 50) train acc: 0.872000; val_acc: 0.652000\n",
      "(Iteration 17641 / 24500) loss: 0.590296\n",
      "(Iteration 17661 / 24500) loss: 0.454660\n",
      "(Iteration 17681 / 24500) loss: 0.548940\n",
      "(Iteration 17701 / 24500) loss: 0.473248\n",
      "(Iteration 17721 / 24500) loss: 0.437927\n",
      "(Iteration 17741 / 24500) loss: 0.641772\n",
      "(Iteration 17761 / 24500) loss: 0.494116\n",
      "(Iteration 17781 / 24500) loss: 0.522305\n",
      "(Iteration 17801 / 24500) loss: 0.479470\n",
      "(Iteration 17821 / 24500) loss: 0.470636\n",
      "(Iteration 17841 / 24500) loss: 0.397713\n",
      "(Iteration 17861 / 24500) loss: 0.573561\n",
      "(Iteration 17881 / 24500) loss: 0.545733\n",
      "(Iteration 17901 / 24500) loss: 0.455003\n",
      "(Iteration 17921 / 24500) loss: 0.525619\n",
      "(Iteration 17941 / 24500) loss: 0.631709\n",
      "(Iteration 17961 / 24500) loss: 0.379591\n",
      "(Iteration 17981 / 24500) loss: 0.455011\n",
      "(Iteration 18001 / 24500) loss: 0.580217\n",
      "(Iteration 18021 / 24500) loss: 0.561361\n",
      "(Iteration 18041 / 24500) loss: 0.626749\n",
      "(Iteration 18061 / 24500) loss: 0.458273\n",
      "(Iteration 18081 / 24500) loss: 0.739734\n",
      "(Iteration 18101 / 24500) loss: 0.708899\n",
      "(Iteration 18121 / 24500) loss: 0.491077\n",
      "(Epoch 37 / 50) train acc: 0.869000; val_acc: 0.658000\n",
      "(Iteration 18141 / 24500) loss: 0.351398\n",
      "(Iteration 18161 / 24500) loss: 0.465579\n",
      "(Iteration 18181 / 24500) loss: 0.392370\n",
      "(Iteration 18201 / 24500) loss: 0.423718\n",
      "(Iteration 18221 / 24500) loss: 0.440625\n",
      "(Iteration 18241 / 24500) loss: 0.618458\n",
      "(Iteration 18261 / 24500) loss: 0.386364\n",
      "(Iteration 18281 / 24500) loss: 0.336700\n",
      "(Iteration 18301 / 24500) loss: 0.537355\n",
      "(Iteration 18321 / 24500) loss: 0.423574\n",
      "(Iteration 18341 / 24500) loss: 0.496486\n",
      "(Iteration 18361 / 24500) loss: 0.518904\n",
      "(Iteration 18381 / 24500) loss: 0.544356\n",
      "(Iteration 18401 / 24500) loss: 0.470046\n",
      "(Iteration 18421 / 24500) loss: 0.449637\n",
      "(Iteration 18441 / 24500) loss: 0.564804\n",
      "(Iteration 18461 / 24500) loss: 0.566263\n",
      "(Iteration 18481 / 24500) loss: 0.495093\n",
      "(Iteration 18501 / 24500) loss: 0.498240\n",
      "(Iteration 18521 / 24500) loss: 0.540125\n",
      "(Iteration 18541 / 24500) loss: 0.548873\n",
      "(Iteration 18561 / 24500) loss: 0.379168\n",
      "(Iteration 18581 / 24500) loss: 0.573057\n",
      "(Iteration 18601 / 24500) loss: 0.508647\n",
      "(Epoch 38 / 50) train acc: 0.872000; val_acc: 0.658000\n",
      "(Iteration 18621 / 24500) loss: 0.512939\n",
      "(Iteration 18641 / 24500) loss: 0.395999\n",
      "(Iteration 18661 / 24500) loss: 0.357873\n",
      "(Iteration 18681 / 24500) loss: 0.433954\n",
      "(Iteration 18701 / 24500) loss: 0.408264\n",
      "(Iteration 18721 / 24500) loss: 0.454756\n",
      "(Iteration 18741 / 24500) loss: 0.381921\n",
      "(Iteration 18761 / 24500) loss: 0.361849\n",
      "(Iteration 18781 / 24500) loss: 0.476861\n",
      "(Iteration 18801 / 24500) loss: 0.606987\n",
      "(Iteration 18821 / 24500) loss: 0.460320\n",
      "(Iteration 18841 / 24500) loss: 0.517564\n",
      "(Iteration 18861 / 24500) loss: 0.662045\n",
      "(Iteration 18881 / 24500) loss: 0.572096\n",
      "(Iteration 18901 / 24500) loss: 0.576880\n",
      "(Iteration 18921 / 24500) loss: 0.591168\n",
      "(Iteration 18941 / 24500) loss: 0.515049\n",
      "(Iteration 18961 / 24500) loss: 0.428729\n",
      "(Iteration 18981 / 24500) loss: 0.644034\n",
      "(Iteration 19001 / 24500) loss: 0.444731\n",
      "(Iteration 19021 / 24500) loss: 0.557077\n",
      "(Iteration 19041 / 24500) loss: 0.450736\n",
      "(Iteration 19061 / 24500) loss: 0.470871\n",
      "(Iteration 19081 / 24500) loss: 0.415413\n",
      "(Iteration 19101 / 24500) loss: 0.495778\n",
      "(Epoch 39 / 50) train acc: 0.875000; val_acc: 0.649000\n",
      "(Iteration 19121 / 24500) loss: 0.380331\n",
      "(Iteration 19141 / 24500) loss: 0.383427\n",
      "(Iteration 19161 / 24500) loss: 0.468769\n",
      "(Iteration 19181 / 24500) loss: 0.538014\n",
      "(Iteration 19201 / 24500) loss: 0.540638\n",
      "(Iteration 19221 / 24500) loss: 0.445777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 19241 / 24500) loss: 0.519043\n",
      "(Iteration 19261 / 24500) loss: 0.332566\n",
      "(Iteration 19281 / 24500) loss: 0.446822\n",
      "(Iteration 19301 / 24500) loss: 0.367259\n",
      "(Iteration 19321 / 24500) loss: 0.507212\n",
      "(Iteration 19341 / 24500) loss: 0.526740\n",
      "(Iteration 19361 / 24500) loss: 0.726906\n",
      "(Iteration 19381 / 24500) loss: 0.450592\n",
      "(Iteration 19401 / 24500) loss: 0.473103\n",
      "(Iteration 19421 / 24500) loss: 0.456177\n",
      "(Iteration 19441 / 24500) loss: 0.498215\n",
      "(Iteration 19461 / 24500) loss: 0.450755\n",
      "(Iteration 19481 / 24500) loss: 0.463410\n",
      "(Iteration 19501 / 24500) loss: 0.599329\n",
      "(Iteration 19521 / 24500) loss: 0.292407\n",
      "(Iteration 19541 / 24500) loss: 0.500995\n",
      "(Iteration 19561 / 24500) loss: 0.329037\n",
      "(Iteration 19581 / 24500) loss: 0.357902\n",
      "(Epoch 40 / 50) train acc: 0.884000; val_acc: 0.654000\n",
      "(Iteration 19601 / 24500) loss: 0.322149\n",
      "(Iteration 19621 / 24500) loss: 0.568842\n",
      "(Iteration 19641 / 24500) loss: 0.510491\n",
      "(Iteration 19661 / 24500) loss: 0.612790\n",
      "(Iteration 19681 / 24500) loss: 0.373004\n",
      "(Iteration 19701 / 24500) loss: 0.341354\n",
      "(Iteration 19721 / 24500) loss: 0.397543\n",
      "(Iteration 19741 / 24500) loss: 0.453396\n",
      "(Iteration 19761 / 24500) loss: 0.438102\n",
      "(Iteration 19781 / 24500) loss: 0.375385\n",
      "(Iteration 19801 / 24500) loss: 0.489190\n",
      "(Iteration 19821 / 24500) loss: 0.496776\n",
      "(Iteration 19841 / 24500) loss: 0.554432\n",
      "(Iteration 19861 / 24500) loss: 0.531637\n",
      "(Iteration 19881 / 24500) loss: 0.437530\n",
      "(Iteration 19901 / 24500) loss: 0.456598\n",
      "(Iteration 19921 / 24500) loss: 0.385127\n",
      "(Iteration 19941 / 24500) loss: 0.521505\n",
      "(Iteration 19961 / 24500) loss: 0.465648\n",
      "(Iteration 19981 / 24500) loss: 0.409351\n",
      "(Iteration 20001 / 24500) loss: 0.317213\n",
      "(Iteration 20021 / 24500) loss: 0.546505\n",
      "(Iteration 20041 / 24500) loss: 0.388677\n",
      "(Iteration 20061 / 24500) loss: 0.527013\n",
      "(Iteration 20081 / 24500) loss: 0.717450\n",
      "(Epoch 41 / 50) train acc: 0.865000; val_acc: 0.651000\n",
      "(Iteration 20101 / 24500) loss: 0.331235\n",
      "(Iteration 20121 / 24500) loss: 0.375474\n",
      "(Iteration 20141 / 24500) loss: 0.414611\n",
      "(Iteration 20161 / 24500) loss: 0.569366\n",
      "(Iteration 20181 / 24500) loss: 0.717784\n",
      "(Iteration 20201 / 24500) loss: 0.374798\n",
      "(Iteration 20221 / 24500) loss: 0.366403\n",
      "(Iteration 20241 / 24500) loss: 0.452179\n",
      "(Iteration 20261 / 24500) loss: 0.534848\n",
      "(Iteration 20281 / 24500) loss: 0.396513\n",
      "(Iteration 20301 / 24500) loss: 0.360454\n",
      "(Iteration 20321 / 24500) loss: 0.561487\n",
      "(Iteration 20341 / 24500) loss: 0.606112\n",
      "(Iteration 20361 / 24500) loss: 0.524855\n",
      "(Iteration 20381 / 24500) loss: 0.470865\n",
      "(Iteration 20401 / 24500) loss: 0.423710\n",
      "(Iteration 20421 / 24500) loss: 0.537296\n",
      "(Iteration 20441 / 24500) loss: 0.446779\n",
      "(Iteration 20461 / 24500) loss: 0.451540\n",
      "(Iteration 20481 / 24500) loss: 0.388086\n",
      "(Iteration 20501 / 24500) loss: 0.378000\n",
      "(Iteration 20521 / 24500) loss: 0.454280\n",
      "(Iteration 20541 / 24500) loss: 0.421702\n",
      "(Iteration 20561 / 24500) loss: 0.567062\n",
      "(Epoch 42 / 50) train acc: 0.882000; val_acc: 0.646000\n",
      "(Iteration 20581 / 24500) loss: 0.396468\n",
      "(Iteration 20601 / 24500) loss: 0.333857\n",
      "(Iteration 20621 / 24500) loss: 0.498927\n",
      "(Iteration 20641 / 24500) loss: 0.386404\n",
      "(Iteration 20661 / 24500) loss: 0.345315\n",
      "(Iteration 20681 / 24500) loss: 0.442388\n",
      "(Iteration 20701 / 24500) loss: 0.522729\n",
      "(Iteration 20721 / 24500) loss: 0.422241\n",
      "(Iteration 20741 / 24500) loss: 0.484095\n",
      "(Iteration 20761 / 24500) loss: 0.435331\n",
      "(Iteration 20781 / 24500) loss: 0.528947\n",
      "(Iteration 20801 / 24500) loss: 0.381191\n",
      "(Iteration 20821 / 24500) loss: 0.320130\n",
      "(Iteration 20841 / 24500) loss: 0.439699\n",
      "(Iteration 20861 / 24500) loss: 0.479028\n",
      "(Iteration 20881 / 24500) loss: 0.625152\n",
      "(Iteration 20901 / 24500) loss: 0.389160\n",
      "(Iteration 20921 / 24500) loss: 0.494120\n",
      "(Iteration 20941 / 24500) loss: 0.572690\n",
      "(Iteration 20961 / 24500) loss: 0.608986\n",
      "(Iteration 20981 / 24500) loss: 0.441510\n",
      "(Iteration 21001 / 24500) loss: 0.357289\n",
      "(Iteration 21021 / 24500) loss: 0.319709\n",
      "(Iteration 21041 / 24500) loss: 0.431623\n",
      "(Iteration 21061 / 24500) loss: 0.616003\n",
      "(Epoch 43 / 50) train acc: 0.911000; val_acc: 0.659000\n",
      "(Iteration 21081 / 24500) loss: 0.408283\n",
      "(Iteration 21101 / 24500) loss: 0.642814\n",
      "(Iteration 21121 / 24500) loss: 0.409055\n",
      "(Iteration 21141 / 24500) loss: 0.317633\n",
      "(Iteration 21161 / 24500) loss: 0.439450\n",
      "(Iteration 21181 / 24500) loss: 0.544551\n",
      "(Iteration 21201 / 24500) loss: 0.366307\n",
      "(Iteration 21221 / 24500) loss: 0.433635\n",
      "(Iteration 21241 / 24500) loss: 0.392882\n",
      "(Iteration 21261 / 24500) loss: 0.338801\n",
      "(Iteration 21281 / 24500) loss: 0.598050\n",
      "(Iteration 21301 / 24500) loss: 0.284553\n",
      "(Iteration 21321 / 24500) loss: 0.424349\n",
      "(Iteration 21341 / 24500) loss: 0.511761\n",
      "(Iteration 21361 / 24500) loss: 0.515280\n",
      "(Iteration 21381 / 24500) loss: 0.391107\n",
      "(Iteration 21401 / 24500) loss: 0.624881\n",
      "(Iteration 21421 / 24500) loss: 0.385701\n",
      "(Iteration 21441 / 24500) loss: 0.375973\n",
      "(Iteration 21461 / 24500) loss: 0.296806\n",
      "(Iteration 21481 / 24500) loss: 0.446317\n",
      "(Iteration 21501 / 24500) loss: 0.416626\n",
      "(Iteration 21521 / 24500) loss: 0.393637\n",
      "(Iteration 21541 / 24500) loss: 0.445820\n",
      "(Epoch 44 / 50) train acc: 0.873000; val_acc: 0.639000\n",
      "(Iteration 21561 / 24500) loss: 0.409787\n",
      "(Iteration 21581 / 24500) loss: 0.451435\n",
      "(Iteration 21601 / 24500) loss: 0.424557\n",
      "(Iteration 21621 / 24500) loss: 0.503722\n",
      "(Iteration 21641 / 24500) loss: 0.337729\n",
      "(Iteration 21661 / 24500) loss: 0.424295\n",
      "(Iteration 21681 / 24500) loss: 0.499947\n",
      "(Iteration 21701 / 24500) loss: 0.587764\n",
      "(Iteration 21721 / 24500) loss: 0.394255\n",
      "(Iteration 21741 / 24500) loss: 0.435879\n",
      "(Iteration 21761 / 24500) loss: 0.435960\n",
      "(Iteration 21781 / 24500) loss: 0.517898\n",
      "(Iteration 21801 / 24500) loss: 0.409775\n",
      "(Iteration 21821 / 24500) loss: 0.437487\n",
      "(Iteration 21841 / 24500) loss: 0.552675\n",
      "(Iteration 21861 / 24500) loss: 0.485545\n",
      "(Iteration 21881 / 24500) loss: 0.537425\n",
      "(Iteration 21901 / 24500) loss: 0.370361\n",
      "(Iteration 21921 / 24500) loss: 0.440687\n",
      "(Iteration 21941 / 24500) loss: 0.452954\n",
      "(Iteration 21961 / 24500) loss: 0.416072\n",
      "(Iteration 21981 / 24500) loss: 0.502247\n",
      "(Iteration 22001 / 24500) loss: 0.324522\n",
      "(Iteration 22021 / 24500) loss: 0.488760\n",
      "(Iteration 22041 / 24500) loss: 0.502585\n",
      "(Epoch 45 / 50) train acc: 0.892000; val_acc: 0.643000\n",
      "(Iteration 22061 / 24500) loss: 0.343861\n",
      "(Iteration 22081 / 24500) loss: 0.341485\n",
      "(Iteration 22101 / 24500) loss: 0.329726\n",
      "(Iteration 22121 / 24500) loss: 0.391130\n",
      "(Iteration 22141 / 24500) loss: 0.370466\n",
      "(Iteration 22161 / 24500) loss: 0.360533\n",
      "(Iteration 22181 / 24500) loss: 0.360616\n",
      "(Iteration 22201 / 24500) loss: 0.362933\n",
      "(Iteration 22221 / 24500) loss: 0.412713\n",
      "(Iteration 22241 / 24500) loss: 0.394352\n",
      "(Iteration 22261 / 24500) loss: 0.371902\n",
      "(Iteration 22281 / 24500) loss: 0.366721\n",
      "(Iteration 22301 / 24500) loss: 0.302358\n",
      "(Iteration 22321 / 24500) loss: 0.486282\n",
      "(Iteration 22341 / 24500) loss: 0.543325\n",
      "(Iteration 22361 / 24500) loss: 0.390888\n",
      "(Iteration 22381 / 24500) loss: 0.494059\n",
      "(Iteration 22401 / 24500) loss: 0.328736\n",
      "(Iteration 22421 / 24500) loss: 0.293617\n",
      "(Iteration 22441 / 24500) loss: 0.410460\n",
      "(Iteration 22461 / 24500) loss: 0.526010\n",
      "(Iteration 22481 / 24500) loss: 0.368658\n",
      "(Iteration 22501 / 24500) loss: 0.383876\n",
      "(Iteration 22521 / 24500) loss: 0.531353\n",
      "(Epoch 46 / 50) train acc: 0.900000; val_acc: 0.647000\n",
      "(Iteration 22541 / 24500) loss: 0.439160\n",
      "(Iteration 22561 / 24500) loss: 0.347642\n",
      "(Iteration 22581 / 24500) loss: 0.324666\n",
      "(Iteration 22601 / 24500) loss: 0.310073\n",
      "(Iteration 22621 / 24500) loss: 0.387396\n",
      "(Iteration 22641 / 24500) loss: 0.397239\n",
      "(Iteration 22661 / 24500) loss: 0.333160\n",
      "(Iteration 22681 / 24500) loss: 0.391506\n",
      "(Iteration 22701 / 24500) loss: 0.451782\n",
      "(Iteration 22721 / 24500) loss: 0.494816\n",
      "(Iteration 22741 / 24500) loss: 0.428422\n",
      "(Iteration 22761 / 24500) loss: 0.442247\n",
      "(Iteration 22781 / 24500) loss: 0.510075\n",
      "(Iteration 22801 / 24500) loss: 0.474742\n",
      "(Iteration 22821 / 24500) loss: 0.354690\n",
      "(Iteration 22841 / 24500) loss: 0.424203\n",
      "(Iteration 22861 / 24500) loss: 0.365241\n",
      "(Iteration 22881 / 24500) loss: 0.366344\n",
      "(Iteration 22901 / 24500) loss: 0.415340\n",
      "(Iteration 22921 / 24500) loss: 0.401821\n",
      "(Iteration 22941 / 24500) loss: 0.461287\n",
      "(Iteration 22961 / 24500) loss: 0.381350\n",
      "(Iteration 22981 / 24500) loss: 0.427640\n",
      "(Iteration 23001 / 24500) loss: 0.555827\n",
      "(Iteration 23021 / 24500) loss: 0.329024\n",
      "(Epoch 47 / 50) train acc: 0.888000; val_acc: 0.625000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 23041 / 24500) loss: 0.399479\n",
      "(Iteration 23061 / 24500) loss: 0.341048\n",
      "(Iteration 23081 / 24500) loss: 0.356532\n",
      "(Iteration 23101 / 24500) loss: 0.323572\n",
      "(Iteration 23121 / 24500) loss: 0.355913\n",
      "(Iteration 23141 / 24500) loss: 0.326789\n",
      "(Iteration 23161 / 24500) loss: 0.340759\n",
      "(Iteration 23181 / 24500) loss: 0.416333\n",
      "(Iteration 23201 / 24500) loss: 0.410128\n",
      "(Iteration 23221 / 24500) loss: 0.348787\n",
      "(Iteration 23241 / 24500) loss: 0.394094\n",
      "(Iteration 23261 / 24500) loss: 0.358133\n",
      "(Iteration 23281 / 24500) loss: 0.276300\n",
      "(Iteration 23301 / 24500) loss: 0.394319\n",
      "(Iteration 23321 / 24500) loss: 0.349235\n",
      "(Iteration 23341 / 24500) loss: 0.371274\n",
      "(Iteration 23361 / 24500) loss: 0.470963\n",
      "(Iteration 23381 / 24500) loss: 0.462993\n",
      "(Iteration 23401 / 24500) loss: 0.262863\n",
      "(Iteration 23421 / 24500) loss: 0.386191\n",
      "(Iteration 23441 / 24500) loss: 0.433305\n",
      "(Iteration 23461 / 24500) loss: 0.313701\n",
      "(Iteration 23481 / 24500) loss: 0.331398\n",
      "(Iteration 23501 / 24500) loss: 0.591868\n",
      "(Epoch 48 / 50) train acc: 0.887000; val_acc: 0.641000\n",
      "(Iteration 23521 / 24500) loss: 0.376182\n",
      "(Iteration 23541 / 24500) loss: 0.330836\n",
      "(Iteration 23561 / 24500) loss: 0.460465\n",
      "(Iteration 23581 / 24500) loss: 0.325686\n",
      "(Iteration 23601 / 24500) loss: 0.352320\n",
      "(Iteration 23621 / 24500) loss: 0.378013\n",
      "(Iteration 23641 / 24500) loss: 0.453754\n",
      "(Iteration 23661 / 24500) loss: 0.508962\n",
      "(Iteration 23681 / 24500) loss: 0.612047\n",
      "(Iteration 23701 / 24500) loss: 0.322796\n",
      "(Iteration 23721 / 24500) loss: 0.292281\n",
      "(Iteration 23741 / 24500) loss: 0.521397\n",
      "(Iteration 23761 / 24500) loss: 0.354894\n",
      "(Iteration 23781 / 24500) loss: 0.238845\n",
      "(Iteration 23801 / 24500) loss: 0.360833\n",
      "(Iteration 23821 / 24500) loss: 0.433326\n",
      "(Iteration 23841 / 24500) loss: 0.413176\n",
      "(Iteration 23861 / 24500) loss: 0.254096\n",
      "(Iteration 23881 / 24500) loss: 0.488594\n",
      "(Iteration 23901 / 24500) loss: 0.387789\n",
      "(Iteration 23921 / 24500) loss: 0.351794\n",
      "(Iteration 23941 / 24500) loss: 0.424153\n",
      "(Iteration 23961 / 24500) loss: 0.473777\n",
      "(Iteration 23981 / 24500) loss: 0.353087\n",
      "(Iteration 24001 / 24500) loss: 0.429031\n",
      "(Epoch 49 / 50) train acc: 0.882000; val_acc: 0.640000\n",
      "(Iteration 24021 / 24500) loss: 0.436578\n",
      "(Iteration 24041 / 24500) loss: 0.293223\n",
      "(Iteration 24061 / 24500) loss: 0.368808\n",
      "(Iteration 24081 / 24500) loss: 0.357254\n",
      "(Iteration 24101 / 24500) loss: 0.459756\n",
      "(Iteration 24121 / 24500) loss: 0.263477\n",
      "(Iteration 24141 / 24500) loss: 0.308730\n",
      "(Iteration 24161 / 24500) loss: 0.384147\n",
      "(Iteration 24181 / 24500) loss: 0.355476\n",
      "(Iteration 24201 / 24500) loss: 0.381417\n",
      "(Iteration 24221 / 24500) loss: 0.407681\n",
      "(Iteration 24241 / 24500) loss: 0.406724\n",
      "(Iteration 24261 / 24500) loss: 0.315189\n",
      "(Iteration 24281 / 24500) loss: 0.412932\n",
      "(Iteration 24301 / 24500) loss: 0.298980\n",
      "(Iteration 24321 / 24500) loss: 0.389248\n",
      "(Iteration 24341 / 24500) loss: 0.351589\n",
      "(Iteration 24361 / 24500) loss: 0.425324\n",
      "(Iteration 24381 / 24500) loss: 0.273327\n",
      "(Iteration 24401 / 24500) loss: 0.357316\n",
      "(Iteration 24421 / 24500) loss: 0.441135\n",
      "(Iteration 24441 / 24500) loss: 0.423455\n",
      "(Iteration 24461 / 24500) loss: 0.388743\n",
      "(Iteration 24481 / 24500) loss: 0.315396\n",
      "(Epoch 50 / 50) train acc: 0.919000; val_acc: 0.638000\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data, \n",
    "                num_epochs=50, batch_size=100,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                  #'learning_rate': 0.002,\n",
    "                },\n",
    "                lr_decay = 0.97,\n",
    "                print_every = 20,\n",
    "                verbose = True)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('D:\\study\\Artificial-Intelligence-Fall-2019\\EXP\\E16_20191225_DL\\loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
